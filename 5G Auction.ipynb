{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with loading all necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a CSV file into the dataframe\n",
    "dataFile = ''\n",
    "\n",
    "tweets_5Gandauction = dataFile\n",
    "\n",
    "tweet_5Gandauction_data = pd.read_csv(tweets_5Gandauction, sep = \",\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the unnecesary columns. In this example I am only interested in the text and need to protect the user information.\n",
    "\n",
    "tweet_5Gandauction_data = tweet_5Gandauction_data.drop(columns =['from_user','id_str','geo_coordinates','created_at','time' ,'user_lang','in_reply_to_user_id_str', 'in_reply_to_screen_name', 'from_user_id_str', 'in_reply_to_status_id_str', 'source','profile_image_url' ,'user_followers_count', 'user_friends_count', 'status_url', 'entities_str' ])\n",
    "tweet_5Gandauction_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints only the text column\n",
    "tweet_5Gandauction_data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts how many tweets (rows) are in my data file\n",
    "tweet_5Gandauction_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK is a platform for building Python programs to work with human language data. \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A token is a piece of a whole, so a word is a token in a sentence, and a sentence is a token in a paragraph. \n",
    "# Tokenization is the process of splitting a string into a list of tokens.\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "word_tokens = tweet_5Gandauction_data['text'].apply(lambda x : tweet_tokenizer.tokenize(x))\n",
    "word_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(x):\n",
    "    return  [words.lower() for words in x]\n",
    "    \n",
    "lower_tokens = word_tokens.apply(lambda x : lower_case(x))\n",
    "lower_tokens.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes usernames from the text column\n",
    "def remove_username(x):\n",
    "    return  [re.sub(r'(?i)@[a-z0-9_]+', \"\", words, flags=re.MULTILINE) for words in x]\n",
    "\n",
    "no_username = lower_tokens.apply(lambda x : remove_username(x))\n",
    "no_username.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes tags inside the text column\n",
    "def remove_tags(x):\n",
    "    return  [re.sub(r'(?i)RT^@[a-z0-9_]+', \"\", words, flags=re.MULTILINE) for words in x]\n",
    "\n",
    "no_retweet_tags = no_username.apply(lambda x : remove_tags(x))\n",
    "no_retweet_tags.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove url inside the text column \n",
    "def remove_url(x):\n",
    "    return  [re.sub(r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)*\\/?\\S', \"\", words, flags=re.MULTILINE) for words in x]\n",
    "\n",
    "no_urls = no_retweet_tags.apply(lambda x : remove_url(x))\n",
    "no_urls.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_lemma(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return  [lemmatizer.lemmatize(words) for words in x]\n",
    "\n",
    "lemmatized_tokens = no_urls.apply(lambda x : get_lemma(x))\n",
    "lemmatized_tokens.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stopwords list in english. A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(x, stop_words):\n",
    "    return [words for words in x if not words in stop_words]\n",
    "\n",
    "    \n",
    "stopwords_removed = lemmatized_tokens.apply(lambda x: remove_stopwords(x, stop_words))\n",
    "stopwords_removed.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(x):\n",
    "    return  [words for words in x if words.isalpha()]\n",
    "\n",
    "no_punct = stopwords_removed.apply(lambda x : remove_punctuations(x))\n",
    "no_punct.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "def tweet_preprocessor(text):\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    clean_text = tweet_tokenizer.tokenize(text)\n",
    "    clean_text = lower_case(clean_text)\n",
    "    clean_text = remove_tags(clean_text)\n",
    "    clean_text = remove_username(clean_text)\n",
    "    clean_text = remove_url(clean_text)\n",
    "    clean_text = remove_stopwords(clean_text, stop_words)\n",
    "    clean_text = get_lemma(clean_text)\n",
    "    clean_text = remove_punctuations(clean_text)\n",
    "    \n",
    "    clean_text = ', '.join(words for words in clean_text)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_5Gandauction_data['clean_text'] = tweet_5Gandauction_data['text'].apply(lambda x : tweet_preprocessor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_5Gandauction_data['clean_text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_5Gandauction_data['word_count'] = tweet_5Gandauction_data['clean_text'].apply(lambda x : len(x))\n",
    "tweet_5Gandauction_data = tweet_5Gandauction_data[tweet_5Gandauction_data['word_count'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_5Gandauction_data['clean_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "pd.options.display.max_colwidth = 1000\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor = tweet_preprocessor,\n",
    "                                   max_df=0.75, min_df=3, use_idf =True)\n",
    "\n",
    "text_tfidf = tfidf_vectorizer.fit_transform(tweet_5Gandauction_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ = np.asarray(text_tfidf.sum(axis=0)).ravel().tolist()\n",
    "tfidf_scores = pd.DataFrame({'terms': tfidf_vectorizer.get_feature_names(), 'tfidf': occ})\n",
    "\n",
    "tfidf_scores.sort_values('tfidf',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count which words are the most popular and how many times are mentioned in the collected tweets.\n",
    "\n",
    "count_vectorizer = CountVectorizer(preprocessor = tweet_preprocessor)\n",
    "count_matrix =count_vectorizer.fit_transform(tweet_5Gandauction_data['text'])\n",
    "\n",
    "occ = np.asarray(count_matrix.sum(axis=0)).ravel().tolist()\n",
    "freq_counts = pd.DataFrame({'terms': count_vectorizer.get_feature_names(), 'Freq': occ})\n",
    "\n",
    "freq_counts.sort_values('Freq',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_5Gandauction_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_5Gandauction_data = tweet_5Gandauction_data.drop(columns ='text')\n",
    "tweet_5Gandauction_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_5Gandauction_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated Tweets\n",
    "\n",
    "tweet_5Gandauction_data = tweet_5Gandauction_data.drop_duplicates(subset='clean_text', keep='first')\n",
    "tweet_5Gandauction_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many tweets we have after removing diplicates\n",
    "tweet_5Gandauction_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count terms frequency after removing duplicates\n",
    "\n",
    "count_vectorizer = CountVectorizer(preprocessor = tweet_preprocessor)\n",
    "count_matrix = count_vectorizer.fit_transform(tweet_5Gandauction_data['clean_text'])\n",
    "\n",
    "occ = np.asarray(count_matrix.sum(axis=0)).ravel().tolist()\n",
    "freq_counts = pd.DataFrame({'terms': count_vectorizer.get_feature_names(), 'Freq': occ})\n",
    "\n",
    "freq_counts.sort_values('Freq',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Wordcloud and Marplot to create our wordcloud.\n",
    "# Here we are putting all the words together and removit rt initials for retweet. \n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "for row in tweet_5Gandauction_data['clean_text']:\n",
    "    row = str(row)\n",
    "    row = row.replace(',', '')\n",
    "    tokens = row.split()\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "        if (tokens[i] != 'rt'):\n",
    "            comment_words += tokens[i] + \" \"\n",
    "        \n",
    "    \n",
    "print(comment_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the wordcloud\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                     background_color = \"white\",\n",
    "                     min_font_size = 10,\n",
    "                     max_words = 100).generate(comment_words)\n",
    "\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the wordcloud image to a destination folder\n",
    "destinationFolder = ''\n",
    "\n",
    "wordcloud.to_file(destinationFolder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
